@article{Weiser1981,
author = {Weiser, Mark},
isbn = {0-89791-146-6},
keywords = {Data flow analysis,Debugging,Human factors,Program maintenance,Program metrics,Software tools},
month = {mar},
pages = {439--449},
publisher = {IEEE Press},
title = {{Program slicing}},
url = {http://dl.acm.org/citation.cfm?id=800078.802557},
year = {1981}
}
@article{Weiser1982,
abstract = {Computer programmers break apart large programs into smaller coherent pieces. Each of these pieces: functions, subroutines, modules, or asbtract datatypes, is usually a contiguous piece of programmers text. The experiment reported here shows that programmers also routinely break programs into one kind of coherent piece which is not contigious. When debugging unfamiliar programs programmers use program pieces called slices which are sets of statements related by their flow of data. The statements in a slice are not necessarily textually contiguous, but may be scattered through a program.},
author = {Weiser, Mark},
doi = {10.1145/358557.358577},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weiser - 1982 - Programmers use slices when debugging.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {7},
pages = {446--452},
title = {{Programmers use slices when debugging}},
volume = {25},
year = {1982}
}
@article{Morell1990,
author = {Morell, L.J.},
doi = {10.1109/32.57623},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Computer science,Differential equations,Information analysis,Performance analysis,Performance evaluation,Testing,alternate expressions,alternative set,computational complexity,fault-based program testing,finite test,prescribed faults,program expressions,program verification,propagation equation,symbol manipulation,symbolic alternative,symbolic execution,test set},
language = {English},
number = {8},
pages = {844--857},
publisher = {IEEE},
title = {{A theory of fault-based testing}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=57623},
volume = {16},
year = {1990}
}
@misc{,
abstract = {Given the central role that software development plays in the delivery and application of information technology, managers are increasingly focusing on process improvement in the software development area. This demand has spurred the provision of a number of new and/or improved approaches to software development, with perhaps the most prominent being object-orientation (OO). In addition, the focus on process improvement has increased the demand for software measures, or metrics with which to manage the process. The need for such metrics is particularly acute when an organization is adopting a new technology for which established practices have yet to be developed. This research addresses these needs through the development and implementation of a new suite of metrics for OO design. Metrics developed in previous research, while contributing to the field's understanding of software development processes, have generally been subject to serious criticisms, including the lack of a theoretical base. Following Wand and Weber (1989), the theoretical base chosen for the metrics was the ontology of Bunge (1977). Six design metrics are developed, and then analytically evaluated against Weyuker's (1988) proposed set of measurement principles. An automated data collection tool was then developed and implemented to collect an empirical sample of these metrics at two field sites in order to demonstrate their feasibility and suggest ways in which managers may use these metrics for process improvement},
booktitle = {Tse},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 1994 - A Metrics Suite for Object Oriented Design.pdf:pdf},
keywords = {Class,complexity,design,management,measurement,metrics,object orientation,performance},
pages = {476--493},
title = {{A Metrics Suite for Object Oriented Design}},
volume = {20},
year = {1994}
}
@article{Reps1997,
abstract = {This paper describes new techniques to help with testing and debugging, using information obtained from path profiling. Apath profiler instruments a program so that the number of times each different loopfree path executes is accumulated during an execution run. With such an instrumented program, each run of the program generates a path spectrum for the execution—a distribution of the paths that were executed during that run. Apath spectrum is a finite, easily obtainable characterization of a program's execution on adataset, and provides a behavior signature for a run of the program. Our techniques are based on the idea of comparing path spectra from different runs of the program. When different runs produce different spectra, the spectral differences can be used to identify paths in the program along which control diverges in the two runs. By choosing input datasets to hold all factors constant except one, the divergence can be attributed to this factor. The point of divergence itself may not be the cause of the underlying problem, but provides a starting place for a programmer to begin his exploration. One application of this technique is in the “Year 2000 Problem ” (i.e., the problem of fixing computer systems that use only 2-digit year fields in date-valued data). In this context, path-spectrum comparison provides a heuristic for identifying paths in a program that are good candidates for being date-dependent computations. The application of path-spectrum comparison to a number of other software-maintenance issues is also discussed.},
author = {Reps, Thomas and Ball, Thomas and Das, Manuvir and Larus, James},
doi = {10.1145/267896.267925},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Reps et al. - 1997 - The use of program profiling for software maintenance with applications to the year 2000 problem.pdf:pdf},
isbn = {3-540-63531-9},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
number = {6},
pages = {432--449},
title = {{The use of program profiling for software maintenance with applications to the year 2000 problem}},
volume = {22},
year = {1997}
}
@article{Mockus2000,
abstract = {Large scale software products must constantly change in order to adapt to a changing environment. Studies of historic data from legacy software systems have identified three specific causes of this change: adding new features; correcting faults; and restructuring code to accommodate future changes. Our hypothesis is that a textual description field of a change is essential to understanding why that change was performed. Also, we expect that difficulty, size, and interval would vary strongly across different types of changes. To test these hypotheses we have designed a program which automatically classifies maintenance activity based on a textual description of changes. Developer surveys showed that the automatic classification was in agreement with developer opinions. Tests of the classifier on a different product found that size and interval for different types of changes did not vary across two products. We have found strong relationships between the type and size of a change and the time required to carry it out. We also discovered a relatively large amount of perfective changes in the system we examined. From this study we have arrived at several suggestions on how to make version control data useful in diagnosing the state of a software project, without significantly increasing the overhead for the developer using the change management system},
author = {Mockus, a. and Votta, L.G.},
doi = {10.1109/ICSM.2000.883028},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mockus, Votta - 2000 - Identifying reasons for software changes using historic databases.pdf:pdf},
isbn = {0-7695-0753-0},
issn = {1063-6773},
journal = {ICSM conf.},
pages = {120--130},
title = {{Identifying reasons for software changes using historic databases}},
year = {2000}
}
@article{Chen2002,
abstract = { Traditional problem determination techniques rely on static dependency models that are difficult to generate accurately in today's large, distributed, and dynamic application environments such as e-commerce systems. We present a dynamic analysis methodology that automates problem determination in these environments by 1) coarse-grained tagging of numerous real client requests as they travel through the system and 2) using data mining techniques to correlate the believed failures and successes of these requests to determine which components are most likely to be at fault. To validate our methodology, we have implemented Pinpoint, a framework for root cause analysis on the J2EE platform that requires no knowledge of the application components. Pinpoint consists of three parts: a communications layer that traces client requests, a failure detector that uses traffic-sniffing and middleware instrumentation, and a data analysis engine. We evaluate Pinpoint by injecting faults into various application components and show that Pinpoint identifies the faulty components with high accuracy and produces few false-positives.},
author = {Chen, Mike Y. and Kiciman, Emre and Fratkin, Eugene and Fox, Armando and Brewer, Eric},
doi = {10.1109/DSN.2002.1029005},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2002 - Pinpoint Problem determination in large, dynamic internet services.pdf:pdf},
isbn = {0769515975},
journal = {Proceedings of the 2002 International Conference on Dependable Systems and Networks},
keywords = {Data clustering,Data mining algorithms,Problem determination,Problem diagnosis,Root cause analysis},
pages = {595--604},
title = {{Pinpoint: Problem determination in large, dynamic internet services}},
year = {2002}
}
@article{Perez2004,
abstract = {Fault tolerant circuits are currently required in several major application sectors. Besides and in complement to other possible approaches such as proving or analytical modeling whose applicability and accuracy are significantly restricted in the case of complex fault tolerant systems, fault-injection has been recognized to be particularly attractive and valuable. Fault injection provides a method of assessing the dependability of a system under test. It involves inserting faults into a system and monitoring the system to determine its behavior in response to a fault. Several fault injection techniques have been proposed and practically experimented. They can be grouped into hardware-based fault injection, software-based fault injection, simulation-based fault injection, emulation-based fault injection and hybrid fault injection. This paper presents a survey on fault injection techniques with comparison of the different injection techniques and an overview on the different tools.},
author = {Perez, Alexandre and Abreu, Rui and Wong, Eric},
doi = {10.1.1.167.966},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perez, Abreu, Wong - 2004 - A Survey on Fault Injection Techniques.pdf:pdf},
isbn = {9781450324281},
issn = {03600300},
keywords = {fault injection,fault injector,fault simulation,fault tolerance,received may 19,vhdl fault models,vlsi circuits},
pages = {171--186},
title = {{A Survey on Fault Injection Techniques}},
volume = {1},
year = {2004}
}
@article{Dallmeier2005,
abstract = { A common method to localize defects is to compare the coverage of passing and failing program runs: A method executed only in failing runs, for instance, is likely to point to the defect. However, some failures, occur only after a specific sequence of method calls, such as multiple deallocations of the same resource. Such sequences can be collected from arbitrary Java programs at low cost; comparing object-specific sequences predicts defects better than simply comparing coverage. In a controlled experiment, our technique pinpointed the defective class in 39{\%} of all test runs.  Title：Lightweight Defect Localization for Java  题目：轻量级Java故障检测 检测故障的常见做法是比较程序成功和失败执行的覆盖：例如，一种方法是失败执行覆盖的部分可能包含着故障。然而，有些故障只有按特定序列执行方法时才回发生，例如，多次释放某些资源。这些执行序列可以低代价的从任意Java程序中收集，与比较覆盖相比，比较特定对象序列可以更好的预测故障。在一次受控实验中，我们的技术找出了39{\%}的测试中的故障。注意： 本文提出的比较特定对象序列的前提是“要预先知道这些对象的序列可以引发故障”（待验证？）},
annote = {AMPLE},
author = {Dallmeier, Valentin and Lindig, Christian and Zeller, Andreas},
doi = {10.1007/11531142_23},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dallmeier, Lindig, Zeller - 2005 - Lightweight Defect Localization for Java.pdf:pdf},
isbn = {978-3-540-27992-1},
issn = {03029743},
journal = {ECOOP 2005 - Object-Oriented Programming,Proceedings},
pages = {528--550},
title = {{Lightweight Defect Localization for Java }},
url = {http://dx.doi.org/10.1007/11531142{\_}23},
volume = {3586},
year = {2005}
}
@article{Jones2005,
abstract = {The high cost of locating faults in programs has motivated the development of techniques that assist in fault localization by automating part of the process of searching for faults. Empirical studies that compare these techniques have reported the relative effectiveness of four existing techniques on a set of subjects. These studies compare the rankings that the techniques compute for statements in the subject programs and the effectiveness of these rankings in locating the faults. However, it is unknown how these four techniques compare with Tarantula, another existing fault-localization technique, although this technique also provides a way to rank statements in terms of their suspiciousness. Thus, we performed a study to compare the Tarantula technique with the four techniques previously compared. This paper presents our study---it overviews the Tarantula technique along with the four other techniques studied, describes our experiment, and reports and discusses the results. Our studies show that, on the same set of subjects, the Tarantula technique consistently outperforms the other four techniques in terms of effectiveness in fault localization, and is comparable in efficiency to the least expensive of the other four techniques.},
author = {Jones, J.a. James a and Harrold, Mary Jean M.J.},
doi = {http://doi.acm.org/10.1145/1101908.1101949},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jones, Harrold - 2005 - Empirical evaluation of the tarantula automatic fault-localization technique.pdf:pdf},
isbn = {1581139934},
journal = {Automated Software Engineering},
keywords = {automated debugging,fault localization,program analysis},
pages = {282--292},
title = {{Empirical evaluation of the tarantula automatic fault-localization technique}},
url = {http://portal.acm.org/citation.cfm?id=1101949},
year = {2005}
}
@article{Sliwerski2005,
author = {{\'{S}}liwerski, Jacek and Zimmermann, Thomas and Zeller, Andreas},
doi = {10.1145/1082983.1083147},
isbn = {1-59593-123-6},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
month = {jul},
number = {4},
pages = {1},
publisher = {ACM},
title = {{When do changes induce fixes?}},
url = {http://dl.acm.org/citation.cfm?id=1082983.1083147},
volume = {30},
year = {2005}
}
@article{++,
abstract = {Finding and fixing software bugs is difficult, and many developers put significant effort into finding and fixing them. A project's software change history records the change that introduces a bug and the change that subsequently fixes it. This bug-introducing and bug-fix experience can be used to predict future bugs. This dissertation presents two bug prediction algorithms that adaptively analyze a project's change history: bug cache and change classification. The basic assumption of the bug cache approach is that the bugs do not occur in isolation, but rather in a burst of several related bugs. The bug cache exploits this locality by caching locations that are likely to have bugs. By consulting the bug cache, a developer can detect locations likely to be fault prone. This is useful for prioritizing verification and validation resources on the most bug prone files, functions, or methods. An evaluation of seven open source projects with more than 200,000 revisions shows that the bug cache selects 10{\%} of the source code files; these files account for 73{\%}-95{\%} of future bugs. The change classification approach learns from previous buggy change patterns using two machine learning algorithms, Na{\"{i}}ve Bayes and Support Vector Machine. After training on buggy change patterns, it predicts new unknown changes as either buggy or clean. As soon as changes are made, developers can use the predicted information to inspect the new changes, which are an average of 20 lines of code. After training on 12 open source projects, the change classification approach can, on average, classify buggy changes with 78{\%} accuracy and 65{\%} buggy change recall. By leveraging project history and learning the unique bug patterns of each project, both approaches can be used to find locations of bugs. This information can be used to increase software quality and reduce software development cost.},
annote = {BugCache
Buggy Change Classification

10, 13, 58, 42
20},
author = {Kim, Sunghun},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim - 2006 - Adaptive Bug Prediction by Analyzing Project History.pdf:pdf},
keywords = {Bug Cache,Change Classification,Classification},
mendeley-tags = {Bug Cache,Change Classification,Classification},
pages = {145},
title = {{Adaptive Bug Prediction by Analyzing Project History}},
year = {2006}
}
@inproceedings{kim2006automatic,
annote = {SZZ ++},
author = {Kim, Sunghun and Zimmermann, Thomas and Pan, Kai and {Whitehead Jr}, E James},
booktitle = {Automated Software Engineering, 2006. ASE'06. 21st IEEE/ACM International Conference on},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2006 - Automatic identification of bug-introducing changes.pdf:pdf},
organization = {IEEE},
pages = {81--90},
title = {{Automatic identification of bug-introducing changes}},
year = {2006}
}
@article{Abreu2007,
abstract = {Spectrum-based fault localization shortens the test- diagnose-repair cycle by reducing the debugging effort. As a light-weight automated diagnosis technique it can easily be integrated with existing testing schemes. However, as no model of the system is taken into account, its diagnostic accuracy is inherently limited. Using the Siemens Set benchmark, we investigate this diagnostic accuracy as a function of several parameters (such as quality and quantity of the program spectra collected during the execution of the system), some of which directly relate to test design. Our results indicate that the superior performance of a particular similarity coefficient, used to analyze the program spectra, is largely independent of test design. Furthermore, near- optimal diagnostic accuracy (exonerating about 80{\%} of the blocks of code on average) is already obtained for low-quality error observations and limited numbers of test cases. The influence of the number of test cases is of primary importance for continuous (embedded) processing applications, where only limited observation horizons can be maintained.},
author = {Abreu, Rui and Zoeteweij, Peter and {Van Gemund}, Arjan J C},
doi = {10.1109/TAICPART.2007.4344104},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abreu, Zoeteweij, Van Gemund - 2007 - On the accuracy of spectrum-based fault localization.pdf:pdf},
isbn = {0769529844},
journal = {Proceedings - Testing: Academic and Industrial Conference Practice and Research Techniques, TAIC PART-Mutation 2007},
keywords = {Program spectra,Software fault diagnosis,Test data analysis},
pages = {89--98},
title = {{On the accuracy of spectrum-based fault localization}},
year = {2007}
}
@inproceedings{Abreu2007a,
author = {Abreu, Rui and Zoeteweij, Peter and van Gemund, Arjan J.C.},
booktitle = {Testing: Academic and Industrial Conference Practice and Research Techniques - MUTATION (TAICPART-MUTATION 2007)},
doi = {10.1109/TAIC.PART.2007.13},
isbn = {0-7695-2984-4},
keywords = {Automatic testing,Benchmark testing,Computer industry,Debugging,Failure analysis,Fault detection,Fault diagnosis,Fault location,Mathematics,Software testing,light-weight automated diagnosis technique,low-quality error observations,program debugging,program diagnostics,program testing,spectrum-based fault localization,test data analysis,test-diagnose-repair cycle},
language = {English},
month = {sep},
pages = {89--98},
publisher = {IEEE},
title = {{On the Accuracy of Spectrum-based Fault Localization}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4344104},
year = {2007}
}
@article{Kim2007,
abstract = {We analyze the version history of 7 software systems to predict the most fault prone entities and files. The basic assumption is that faults do not occur in isolation, but rather in bursts of several related faults. Therefore, we cache locations that are likely to have faults: starting from the location of a known (fixed) fault, we cache the location itself, any locations changed together with the fault, recently added locations, and recently changed locations. By consulting the cache at the moment a fault is fixed, a developer can detect likely fault-prone locations. This is useful for prioritizing verification and validation resources on the most fault prone files or entities. In our evaluation of seven open source projects with more than 200,000 revisions, the cache selects 10{\%} of the source code files; these files account for 73{\%}-95{\%} of faults - a significant advance beyond the state of the art.},
annote = {BugCache + FixCache},
author = {Kim, Sunghun and Zimmermann, Thomas and Whitehead, E. James and Zeller, Andreas},
doi = {10.1109/ICSE.2007.66},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2007 - Predicting faults from cached history.pdf:pdf},
isbn = {0769528287},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
pages = {489--498},
title = {{Predicting faults from cached history}},
year = {2007}
}
@article{Menzies2007,
annote = {Abordagens {\`{a}} previs{\~{a}}o de defeitos
Sugerido por Alexandre Perez},
author = {Menzies, Tim and Greenwald, Jeremy and Frank, Art},
doi = {10.1109/TSE.2007.256941},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Menzies, Greenwald, Frank - 2007 - Data Mining Static Code Attributes to Learn Defect Predictors.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Art,Artificial intelligence,Bayesian methods,Data mining,Data mining detect prediction,Financial management,Halstead,Learning systems,McCabe,Software quality,Software systems,Software testing,System testing,artifical intelligence,empirical,naive Bayes.},
language = {English},
month = {jan},
number = {1},
pages = {2--13},
publisher = {IEEE},
title = {{Data Mining Static Code Attributes to Learn Defect Predictors}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4027145},
volume = {33},
year = {2007}
}
@article{Mayer2008,
author = {Mayer, Wolfgang and Stumptner, Markus},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mayer, Stumptner - 2008 - Evaluating Models for Model-Based Debugging.pdf.pdf:pdf},
isbn = {9781424421886},
pages = {128--137},
title = {{Evaluating Models for Model-Based Debugging.pdf}},
year = {2008}
}
@article{Whitehead2008,
annote = {Buggy Change Classification},
author = {Whitehead, E.J.},
doi = {10.1109/TSE.2007.70773},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Clustering,Configuration Management,Data mining,Metrics/Measurement,Software maintenance,and association rules,association rule,change classification,classification,data mining,feature extraction,learning (artificial intelligence),machine learning classifier,open source projects,program debugging,programming languages,software change,software configuration management repository,software maintenance,software metrics,software project},
language = {English},
month = {mar},
number = {2},
pages = {181--196},
publisher = {IEEE},
title = {{Classifying Software Changes: Clean or Buggy?}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4408585},
volume = {34},
year = {2008}
}
@article{Abreu2009,
abstract = {Fault diagnosis approaches can generally be categorized into spectrum-based fault localization (SFL, correlating failures with abstractions of program traces), and model-based diagnosis (MBD, logic reasoning over a behavioral model). Although MBD approaches are inherently more accurate than SFL, their high computational complexity prohibits application to large programs. We present a framework to combine the best of both worlds, coined BARINEL. The program is modeled using abstractions of program traces (as in SFL) while Bayesian reasoning is used to deduce multiple-fault candidates and their probabilities (as in MBD). A particular feature of BARINEL is the usage of a probabilistic component model that accounts for the fact that faulty components may fail intermittently. Experimental results on both synthetic and real software programs show that BARINEL typically outperforms current SFL approaches at a cost complexity that is only marginally higher. In the context of single faults this superiority is established by formal proof.},
annote = {Barinel!!},
author = {Abreu, Rui and Zoeteweij, P and Gemund, a J C Van},
doi = {10.1109/ASE.2009.25},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abreu, Zoeteweij, Gemund - 2009 - Spectrum-Based Multiple Fault Localization.pdf:pdf},
isbn = {978-1-4244-5259-0},
issn = {1938-4300},
journal = {Automated Software Engineering 2009 ASE 09 24th IEEEACM International Conference on},
keywords = {program spectra,software fault diagnosis,statistical reasoning approaches},
pages = {88--99},
title = {{Spectrum-Based Multiple Fault Localization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5431781},
year = {2009}
}
@inproceedings{Janssen2009,
abstract = {Locating software components which are responsible for ob- served failures is the most expensive, error-prone phase in the software development life cycle. Automated diagnosis of software faults can improve the efficiency of the debugging process, and is therefore an important process for the de- velopment of dependable software. In this paper we present a toolset for automatic fault localization, dubbed Zoltar, which adopts a spectrum-based fault localization technique. The toolset provides the infrastructure to automatically in- strument the source code of software programs to produce runtime data, which is subsequently analyzed to return a ranked list of likely faulty locations. Aimed at total au- tomation (e.g., for runtime fault diagnosis), Zoltar has the capability of instrumenting the program under analysis with fault screeners, for automatic error detection. Using a small thread-based example program as well as a large realistic program, we show the applicability of the proposed toolset.},
annote = {Zoltar},
author = {Janssen, T and Gemund, A and Abreu, Rui},
booktitle = {Instrumentation},
doi = {10.1145/1596495.1596502},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Janssen, Gemund, Abreu - 2009 - Zoltar A Spectrum-based Fault Localization Tool.pdf:pdf},
isbn = {9781605586816},
pages = {23--29},
title = {{Zoltar: A Spectrum-based Fault Localization Tool}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-70450260945{\&}partnerID=40{\&}md5=7f1ad847cdab93c6b6c96d1e60e1ae78},
year = {2009}
}
@article{RuiAbreu,
annote = {Staccato (Barinel)},
author = {{Rui Abreu}, Arjan J. C. van Gemund},
title = {{A Low-Cost Approximate Minimal Hitting Set Algorithm and its Application to Model-Based Diagnosis}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.169.1046},
year = {2009}
}
@article{Shivaji2009,
annote = {Analisa Change Classification},
author = {Shivaji, Shivkumar and Jr., E. James Whitehead and Akella, Ram and Kim, Sunghun},
doi = {10.1109/ASE.2009.76},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shivaji et al. - 2009 - Reducing Features to Improve Bug Prediction.pdf:pdf},
isbn = {978-1-4244-5259-0},
journal = {2009 IEEE/ACM International Conference on Automated Software Engineering},
number = {Section II},
pages = {600--604},
title = {{Reducing Features to Improve Bug Prediction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5431727},
year = {2009}
}
@article{Zhivich2009,
abstract = {Software is no longer creeping into every aspect of our lives - it's already there. In fact, failing to recognize just how much everything we do depends on software functioning correctly makes modern society vulnerable to software errors.},
author = {Zhivich, Michael and Cunningham, Robert K.},
doi = {10.1109/MSP.2009.56},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhivich, Cunningham - 2009 - The real cost of software errors.pdf:pdf},
isbn = {1521-9615},
issn = {15407993},
journal = {IEEE Security and Privacy},
keywords = {Cost,Risks,Secure systems,Software errors,Software failures},
number = {2},
pages = {87--90},
title = {{The real cost of software errors}},
volume = {7},
year = {2009}
}
@article{Nagappan2010,
abstract = {In software development, every change induces a risk. What happens if code changes again and again in some period of time? In an empirical study on Windows Vista, we found that the features of such change bursts have the highest predictive power for defect-prone components. With precision and recall values well above 90{\%}, change bursts significantly improve upon earlier predictors such as complexity metrics, code churn, or organizational structure. As they only rely on version history and a controlled change process, change bursts are straight-forward to detect and deploy.},
annote = {Features of such change bursts have the highest predictive power for defect-prone components. With precision and recall values well above 90{\%}},
author = {Nagappan, Nachiappan and Zeller, Andreas and Zimmermann, Thomas and Herzig, Kim and Murphy, Brendan},
doi = {10.1109/ISSRE.2010.25},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nagappan et al. - 2010 - Change bursts as defect predictors.pdf:pdf},
isbn = {9780769542553},
issn = {10719458},
journal = {Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
keywords = {Change history,Defects,Developers,Empirical studies,Process metrics,Product metrics,Software mining,Software quality assurance,Version control},
pages = {309--318},
title = {{Change bursts as defect predictors}},
year = {2010}
}
@article{Zhang2010,
abstract = {Quality control charts, especially c-charts, can help monitor software quality evolution for defects over time. c-charts of the Eclipse and Gnome systems showed that for systems experiencing active maintenance and updates, quality evolution is complicated and dynamic. The authors identify six quality evolution patterns and describe their implications. Quality assurance teams can use c-charts and patterns to monitor quality evolution and prioritize their efforts.},
author = {Zhang, Hongyu and Kim, Sunghun},
doi = {10.1109/MS.2010.66},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Kim - 2010 - Monitoring software quality evolution for defects.pdf:pdf},
isbn = {0740-7459 VO  - 27},
issn = {07407459},
journal = {IEEE Software},
keywords = {Maintenance management,Quality evolution,Software engineering,Software quality,Software quality assurance,Statistical process control},
number = {4},
pages = {58--64},
title = {{Monitoring software quality evolution for defects}},
volume = {27},
year = {2010}
}
@article{Abreu2011,
abstract = {(Semi-)automated diagnosis of software faults can drastically increase debugging efficiency, improving reliability and time-to-market. Current automatic diagnosis techniques are predominantly of a statistical nature and, despite typical defect densities, do not explicitly consider multiple faults, as also demonstrated by the popularity of the single-fault benchmark set of programs. We present a reasoning approach, called Zoltar-M(ultiple fault), that yields multiple-fault diagnoses, ranked in order of their probability. Although application of Zoltar-M to programs with many faults requires heuristics (trading-off completeness) to reduce the inherent computational complexity, theory as well as experiments on synthetic program models and multiple-fault program versions available from the software infrastructure repository (SIR) show that for multiple-fault programs this approach can outperform statistical techniques, notably spectrum-based fault localization (SFL). As a side-effect of this research, we present a new SFL variant, called Zoltar-S(ingle fault), that is optimal for single-fault programs, outperforming all other variants known to date. {\textcopyright} 2010 Elsevier Inc. All rights reserved.},
annote = {Zoltar-M},
author = {Abreu, Rui and Zoeteweij, Peter and {Van Gemund}, Arjan J C},
doi = {10.1016/j.jss.2010.11.915},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abreu, Zoeteweij, Van Gemund - 2011 - Simultaneous debugging of software faults(2).pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Program spectra,Software fault diagnosis,Statistical and reasoning approaches},
number = {4},
pages = {573--586},
title = {{Simultaneous debugging of software faults}},
url = {http://dx.doi.org/10.1016/j.jss.2010.11.915},
volume = {84},
year = {2011}
}
@article{Kim2011,
abstract = {Many software defect prediction models have been built using historical defect data obtained by mining software repositories (MSR). Recent studies have discovered that data so collected contain noises because current defect collection practices are based on optional bug fix keywords or bug report links in change logs. Automatically collected defect data based on the change logs could include noises. This paper proposes approaches to deal with the noise in defect data. First, we measure the impact of noise on defect prediction models and provide guidelines for acceptable noise level. We measure noise resistant ability of two well-known defect prediction algorithms and find that in general, for large defect datasets, adding FP (false positive) or FN (false negative) noises alone does not lead to substantial performance differences. However, the prediction performance decreases significantly when the dataset contains 20{\&}{\#}x025;-35{\&}{\#}x025; of both FP and FN noises. Second, we propose a noise detection and elimination algorithm to address this problem. Our empirical study shows that our algorithm can identify noisy instances with reasonable accuracy. In addition, after eliminating the noises using our algorithm, defect prediction accuracy is improved.},
annote = {Reduce noise. 
Reduce FP {\&} FN.
For MSR},
author = {Kim, Sunghun and Zhang, Hongyu and Wu, Rongxin and Gong, Liang},
doi = {10.1145/1985793.1985859},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2011 - Dealing with noise in defect prediction.pdf:pdf},
isbn = {978-1-4503-0445-0},
issn = {0270-5257},
journal = {2011 33rd International Conference on Software Engineering (ICSE)},
keywords = {buggy changes,buggy files,data quality,defect prediction,noise resistance},
pages = {481--490},
title = {{Dealing with noise in defect prediction}},
year = {2011}
}
@inproceedings{Campos2012,
address = {New York, New York, USA},
author = {Campos, Jos{\'{e}} and Riboira, Andr{\'{e}} and Perez, Alexandre and Abreu, Rui},
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering - ASE 2012},
doi = {10.1145/2351676.2351752},
isbn = {9781450312042},
keywords = {Automatic Debugging,Automatic Testing,Eclipse plug-in,GZoltar,RZoltar},
month = {sep},
pages = {378},
publisher = {ACM Press},
title = {{GZoltar: an eclipse plug-in for testing and debugging}},
url = {http://dl.acm.org/citation.cfm?id=2351676.2351752},
year = {2012}
}
@inproceedings{Gousios2012,
abstract = {A common requirement of many empirical software engineering studies is the acquisition and curation of data from software repositories. During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve both the commits to the projects' repositories and events generated through user actions on project resources. GHTorrent aims to create a scalable off line mirror of GitHub's event streams and persistent data, and offer it to the research community as a service. In this paper, we present the project's design and initial implementation and demonstrate how the provided datasets can be queried and processed.},
author = {Gousios, Georgios and Spinellis, D.},
booktitle = {2012 9th IEEE Working Conference on Mining Software Repositories (MSR)},
doi = {10.1109/MSR.2012.6224294},
isbn = {978-1-4673-1761-0},
issn = {2160-1852},
keywords = {Communities,Distributed databases,Electronic mail,GHTorrent,GitHub,Github data,Organizations,Peer to peer computing,Protocols,REST API,application program interfaces,collaboration platform,commits,data acquisition,data curation,dataset,events,mirroring platform,open source software,project hosting platform,project resources,public domain software,repository,software engineering,software engineering studies,software repositories,storage management,user actions},
month = {jun},
pages = {12--21},
publisher = {IEEE},
shorttitle = {Mining Software Repositories (MSR), 2012 9th IEEE },
title = {{GHTorrent: Github's data from a firehose}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6224294},
year = {2012}
}
@article{Hall2012,
abstract = {Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively. {\textcopyright} 2012 IEEE.},
annote = {Abordagens {\`{a}} previs{\~{a}}o de defeitos
Sugerido por Alexandre Perez},
author = {Hall, Tracy and Beecham, Sarah and Bowes, David and Gray, David and Counsell, Steve},
doi = {10.1109/TSE.2011.103},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hall et al. - 2012 - A systematic literature review on fault prediction performance in software engineering.pdf:pdf},
isbn = {9781612081656},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Systematic literature review,software fault prediction},
number = {6},
pages = {1276--1304},
title = {{A systematic literature review on fault prediction performance in software engineering}},
volume = {38},
year = {2012}
}
@article{Miao2012,
author = {Miao, Yi and Chen, Zhenyu and Li, Sihan and Zhao, Zhihong and Zhou, Yuming},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miao et al. - 2012 - Identifying Coincidental Correctness for Fault Localization by Clustering Test Cases.pdf:pdf},
isbn = {1-891706-31-4},
journal = {Seke},
keywords = {- coincidental correctness,cluster analysis,fault},
title = {{Identifying Coincidental Correctness for Fault Localization by Clustering Test Cases.}},
url = {http://software.nju.edu.cn/zychen/paper/2012SEKE1.pdf},
year = {2012}
}
@article{Cardoso2013,
author = {Cardoso, Nuno and Abreu, Rui},
title = {{MHS2: A Map-Reduce Heuristic-Driven Minimal Hitting Set Search Algorithm}},
url = {http://haslab.uminho.pt/ruimaranhao/publications/mhs2-map-reduce-heuristic-driven-minimal-hitting-set-search-algorithm},
year = {2013}
}
@misc{Carlsson2013,
abstract = {When performing an analysis of the evolution of software quality and software metrics,there is a need to get access to as many versions of the source code as possible. There isa lack of research on ...},
author = {Carlsson, Emil},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Carlsson - 2013 - Mining Git Repositories An introduction to repository mining.pdf:pdf},
keywords = {Computer Science,Datavetenskap (datalogi)},
language = {eng},
title = {{Mining Git Repositories : An introduction to repository mining}},
url = {http://www.diva-portal.org/smash/record.jsf?pid=diva2{\%}3A638844{\&}dswid=4999},
year = {2013}
}
@article{Gouveia2013,
abstract = {Testing and debugging is the most expensive, error-prone phase in the software development life cycle. Automated software fault localization can drastically improve the efficiency of this phase, thus improving the overall quality of the software. Amongst the most well-known techniques, due to its efficiency and effectiveness, is spectrum-based fault localization. In this paper, we propose three dynamic graphical forms using HTML5 to display the diagnostic reports yielded by spectrum-based fault localization. The visualizations proposed, namely Sunburst, Vertical Partition, and Bubble Hierarchy, have been implemented within the GZOLTAR toolset, replacing previous and less-intuitive OpenGL-based visualizations. The GZOLTAR toolset is a plug-and-play plugin for the Eclipse IDE to ease world-wide adoption. Finally, we performed an user study with GZOLTAR and confirmed that the visualizations help to drastically reduce the time needed in debugging (e.g., all participants using the visualizations were able to pinpoint the fault, whereas of those using traditional methods only 35{\%} found the fault). The group that used the visualizations took on average 9 minutes and 17 seconds less than the group that did not use them.},
author = {Gouveia, Carlos and Campos, Jos{\'{e}} and Abreu, Rui},
doi = {10.1109/VISSOFT.2013.6650539},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gouveia, Campos, Abreu - 2013 - Using HTML5 visualizations in software fault localization.pdf:pdf},
isbn = {9781479914579},
journal = {2013 1st IEEE Working Conference on Software Visualization - Proceedings of VISSOFT 2013},
keywords = {Automatic Debugging,GZOLTAR,Reports,Visualizations},
title = {{Using HTML5 visualizations in software fault localization}},
year = {2013}
}
@article{Herzig2013a,
abstract = {When analyzing version histories, researchers traditionally focused on single events: e.g. the change that causes a bug, the fix that resolves an issue. Sometimes however, there are indirect effects that count: Changing a module may lead to plenty of follow-up modifications in other places, making the initial change having an impact on those later changes. To this end, we group changes into change genealogies, graphs of changes reflecting their mutual dependencies and influences and develop new metrics to capture the spatial and temporal influence of changes. In this paper, we show that change genealogies offer good classification models when identifying defective source files: With a median precision of 73{\%} and a median recall of 76{\%}, change genealogy defect prediction models not only show better classification accuracies as models based on code complexity, but can also outperform classification models based on code dependency network metrics.},
author = {Herzig, Kim and Just, Sascha and Rau, Andreas and Zeller, Andreas},
doi = {10.1109/ISSRE.2013.6698911},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Herzig et al. - 2013 - Predicting defects using change genealogies.pdf:pdf},
isbn = {9781479923663},
journal = {2013 IEEE 24th International Symposium on Software Reliability Engineering, ISSRE 2013},
keywords = {Data mining,Predictive models,Software engineering,Software qual-ity},
pages = {118--127},
title = {{Predicting defects using change genealogies}},
year = {2013}
}
@article{Herzig2013,
abstract = {When interacting with version control systems, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing the version history, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found up to 15{\%} of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6{\%} of all source files are incorrectly associated with bug reports. We recommend better change organization to limit the impact of tangled changes.},
annote = {15{\%} of all bug fixes consist of multiple tangled changes
16.66{\%} of all source files are incorrectly associated with bug reports.},
author = {Herzig, Kim and Zeller, Andreas},
doi = {10.1109/MSR.2013.6624018},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Herzig, Zeller - 2013 - The impact of tangled code changes.pdf:pdf},
isbn = {9781467329361},
issn = {21601852},
journal = {IEEE International Working Conference on Mining Software Repositories},
keywords = {Bias,Data quality,Mining software repositories,Noise,Tangled code changes},
pages = {121--130},
title = {{The impact of tangled code changes}},
year = {2013}
}
@article{Kim2013,
abstract = {To support developers in debugging and locating bugs, we propose a two-phase prediction model that uses bug reports' contents to suggest the files likely to be fixed. In the first phase, our model checks whether the given bug report contains sufficient information for prediction. If so, the model proceeds to predict files to be fixed, based on the content of the bug report. In other words, our two-phase model "speaks up" only if it is confident of making a suggestion for the given bug report; otherwise, it remains silent. In the evaluation on the Mozilla "Firefox" and "Core" packages, the two-phase model was able to make predictions for almost half of all bug reports; on average, 70 percent of these predictions pointed to the correct files. In addition, we compared the two-phase model with three other prediction models: the Usual Suspects, the one-phase model, and BugScout. The two-phase model manifests the best prediction performance.},
annote = {Interesting for the two-phase approach and the state the

Maybe See: Usual suspects and BugScout

See: 18, 19, 20, 21, 22, 23, 24, 25},
author = {Kim, Dongsun and Tao, Yida and Kim, Sunghun and Zeller, Andreas},
doi = {10.1109/TSE.2013.24},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2013 - Where Should We Fix This Bug A Two-Phase Recommendation Model.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Bug reports,BugScout,Computational modeling,Computer bugs,Core packages,Data mining,Feature extraction,Firefox packages,Mozilla packages,Noise,Predictive models,Software,bug report,debugging,formal verification,machine learning,patch file prediction,program debugging,speaks up,two-phase model,two-phase prediction model,two-phase recommendation model},
month = {nov},
number = {11},
pages = {1597--1610},
shorttitle = {Software Engineering, IEEE Transactions on},
title = {{Where Should We Fix This Bug? A Two-Phase Recommendation Model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6517844},
volume = {39},
year = {2013}
}
@inproceedings{Servant2013,
author = {Servant, Francisco and Jones, James A.},
booktitle = {2013 First IEEE Working Conference on Software Visualization (VISSOFT)},
doi = {10.1109/VISSOFT.2013.6650547},
isbn = {978-1-4799-1457-9},
keywords = {Conferences,Data mining,History,Navigation,Software systems,Visualization,change history,chronos,data visualisation,high level view,historical change events,identifying design rationale,low level view,pattern recognition,queried code,reverse engineering,revision control system tools,semantic comprehension,source code history,source coding,user interfaces,visualization,visualizing slices,zoom able user interface},
language = {English},
month = {sep},
pages = {1--4},
publisher = {IEEE},
title = {{Chronos: Visualizing slices of source-code history}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6650547},
year = {2013}
}
@article{Masri2014,
abstract = {Researchers have argued that for failure to be observed the following three conditions must be met: CR {\&}equals; the defect was reached; CI {\&}equals; the program has transitioned into an infectious state; and CP {\&}equals; the infection has propagated to the output. Coincidental Correctness (CC) arises when the program produces the correct output while condition CR is met but not CP. We recognize two forms of coincidental correctness, weak and strong. In weak CC, CR is met, whereas CI might or might not be met, whereas in strong CC, both CR and CI are met. In this work we first show that CC is prevalent in both of its forms and demonstrate that it is a safety reducing factor for Coverage-Based Fault Localization (CBFL). We then propose two techniques for cleansing test suites from coincidental correctness to enhance CBFL, given that the test cases have already been classified as failing or passing. We evaluated the effectiveness of our techniques by empirically quantifying their accuracy in identifying weak CC tests. The results were promising, for example, the better performing technique, using 105 test suites and statement coverage, exhibited 9{\&}percnt; false negatives, 30{\&}percnt; false positives, and no false negatives nor false positives in 14.3{\&}percnt; of the test suites. Also using 73 test suites and more complex coverage, the numbers were 12{\&}percnt;, 19{\&}percnt;, and 15{\&}percnt;, respectively. },
author = {Masri, Wes and Assi, Rawad Abou},
doi = {10.1145/2559932},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Masri, Assi - 2014 - Prevalence of Coincidental Correctness and Mitigation of Its Impact on Fault Localization.pdf:pdf},
issn = {1049-331X},
journal = {ACM Trans. Softw. Eng. Methodol.},
keywords = {Coincidental correctness,cluster analysis,coverage-based fault localization,fuzzy sets,strong coincidental correctness,weak coincidental correctness},
number = {1},
pages = {8:1----8:28},
title = {{Prevalence of Coincidental Correctness and Mitigation of Its Impact on Fault Localization}},
url = {http://doi.acm.org/10.1145/2559932},
volume = {23},
year = {2014}
}
@article{Xue2014,
author = {Xue, Xiaozhen and Pang, Yulei and Namin, Akbar Siami},
doi = {10.1109/COMPSAC.2014.32},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xue, Pang, Namin - 2014 - Trimming Test Suites with Coincidentally Correct Test Cases for Enhancing Fault Localizations.pdf:pdf},
isbn = {978-1-4799-3575-8},
journal = {2014 IEEE 38th Annual Computer Software and Applications Conference},
keywords = {based on fault localizations,cases with coincidentally correct,coincidentally,correct,coverage-based faults localization,ensemble learning,property negatively affects,support vector machine,the performance of debugging},
pages = {239--244},
title = {{Trimming Test Suites with Coincidentally Correct Test Cases for Enhancing Fault Localizations}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6899222},
year = {2014}
}
@article{Freitas2015,
author = {Freitas, Andr{\'{e}}},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Freitas - 2015 - Software Repository Mining Analytics to Estimate Software Component Reliability.pdf:pdf},
title = {{Software Repository Mining Analytics to Estimate Software Component Reliability}},
year = {2015}
}
@article{Cardoso,
author = {Cardoso, Nuno and Abreu, Rui},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cardoso, Abreu - Unknown - A Kernel Density Estimate-Based Approach to Component Goodness Modeling.pdf:pdf},
isbn = {9781577356158},
keywords = {Technical Track},
pages = {152--158},
title = {{A Kernel Density Estimate-Based Approach to Component Goodness Modeling}}
}
@article{Elmishali,
annote = {Abordagens {\`{a}} previs{\~{a}}o de defeitos
Sugerido por Alexandre Perez},
author = {Elmishali, Amir and Stern, Roni and Kalech, Meir},
file = {:home/andreduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elmishali, Stern, Kalech - Unknown - Data-Augmented Software Diagnosis.pdf:pdf},
pages = {247--252},
title = {{Data-Augmented Software Diagnosis}},
volume = {1},
year = {2015}
}
@misc{,
title = {{Does Bug Prediction Support Human Developers? Findings from a Google Case Study}},
url = {http://research.google.com/pubs/pub41145.html},
urldate = {2016-02-04}
}
