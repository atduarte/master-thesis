@article{RuiAbreu,
annote = {Staccato (Barinel)},
author = {{Rui Abreu}, Arjan J. C. van Gemund},
title = {{A Low-Cost Approximate Minimal Hitting Set Algorithm and its Application to Model-Based Diagnosis}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.169.1046}
}
@article{Perez2004,
abstract = {Fault tolerant circuits are currently required in several major application sectors. Besides and in complement to other possible approaches such as proving or analytical modeling whose applicability and accuracy are significantly restricted in the case of complex fault tolerant systems, fault-injection has been recognized to be particularly attractive and valuable. Fault injection provides a method of assessing the dependability of a system under test. It involves inserting faults into a system and monitoring the system to determine its behavior in response to a fault. Several fault injection techniques have been proposed and practically experimented. They can be grouped into hardware-based fault injection, software-based fault injection, simulation-based fault injection, emulation-based fault injection and hybrid fault injection. This paper presents a survey on fault injection techniques with comparison of the different injection techniques and an overview on the different tools.},
author = {Perez, Alexandre and Abreu, Rui and Wong, Eric},
doi = {10.1.1.167.966},
file = {:home/aduarte/Downloads/105338.pdf:pdf},
isbn = {9781450324281},
issn = {03600300},
keywords = {fault injection,fault injector,fault simulation,fault tolerance,received may 19,vhdl fault models,vlsi circuits},
pages = {171--186},
title = {{A Survey on Fault Injection Techniques}},
volume = {1},
year = {2004}
}
@article{Morell1990,
author = {Morell, L.J.},
doi = {10.1109/32.57623},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Computer science,Differential equations,Information analysis,Performance analysis,Performance evaluation,Testing,alternate expressions,alternative set,computational complexity,fault-based program testing,finite test,prescribed faults,program expressions,program verification,propagation equation,symbol manipulation,symbolic alternative,symbolic execution,test set},
language = {English},
number = {8},
pages = {844--857},
publisher = {IEEE},
title = {{A theory of fault-based testing}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=57623},
volume = {16},
year = {1990}
}
@article{Kim2006,
abstract = {Finding and fixing software bugs is difficult, and many developers put significant effort into finding and fixing them. A project’s software change history records the change that introduces a bug and the change that subsequently fixes it. This bug-introducing and bug-fix experience can be used to predict future bugs. This dissertation presents two bug prediction algorithms that adaptively analyze a project’s change history: bug cache and change classification. The basic assumption of the bug cache approach is that the bugs do not occur in isolation, but rather in a burst of several related bugs. The bug cache exploits this locality by caching locations that are likely to have bugs. By consulting the bug cache, a developer can detect locations likely to be fault prone. This is useful for prioritizing verification and validation resources on the most bug prone files, functions, or methods. An evaluation of seven open source projects with more than 200,000 revisions shows that the bug cache selects 10{\%} of the source code files; these files account for 73{\%}-95{\%} of future bugs. The change classification approach learns from previous buggy change patterns using two machine learning algorithms, Na{\"{\i}}ve Bayes and Support Vector Machine. After training on buggy change patterns, it predicts new unknown changes as either buggy or clean. As soon as changes are made, developers can use the predicted information to inspect the new changes, which are an average of 20 lines of code. After training on 12 open source projects, the change classification approach can, on average, classify buggy changes with 78{\%} accuracy and 65{\%} buggy change recall. By leveraging project history and learning the unique bug patterns of each project, both approaches can be used to find locations of bugs. This information can be used to increase software quality and reduce software development cost.},
annote = {BugCache
Buggy Change Classification

10, 13, 58, 42
20},
author = {Kim, Sunghun},
file = {:home/aduarte/Dropbox/Disserta{\c{c}}{\~{a}}o/Research/Isaac Councill, Adaptive Bug Prediction By Analysing Project History.pdf:pdf},
keywords = {Bug Cache,Change Classification,Classification},
mendeley-tags = {Bug Cache,Change Classification,Classification},
pages = {145},
title = {{Adaptive Bug Prediction by Analyzing Project History}},
year = {2006}
}
@inproceedings{kim2006automatic,
annote = {SZZ ++},
author = {Kim, Sunghun and Zimmermann, Thomas and Pan, Kai and {Whitehead Jr}, E James},
booktitle = {Automated Software Engineering, 2006. ASE'06. 21st IEEE/ACM International Conference on},
file = {:home/aduarte/Downloads/Papers{\_}kim2006ase.pdf:pdf},
organization = {IEEE},
pages = {81--90},
title = {{Automatic identification of bug-introducing changes}},
year = {2006}
}
@article{Whitehead2008,
annote = {Buggy Change Classification},
author = {Whitehead, E.J.},
doi = {10.1109/TSE.2007.70773},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Clustering,Configuration Management,Data mining,Metrics/Measurement,Software maintenance,and association rules,association rule,change classification,classification,data mining,feature extraction,learning (artificial intelligence),machine learning classifier,open source projects,program debugging,programming languages,software change,software configuration management repository,software maintenance,software metrics,software project},
language = {English},
month = {mar},
number = {2},
pages = {181--196},
publisher = {IEEE},
title = {{Classifying Software Changes: Clean or Buggy?}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4408585},
volume = {34},
year = {2008}
}
@article{Jones2005,
abstract = {The high cost of locating faults in programs has motivated the development of techniques that assist in fault localization by automating part of the process of searching for faults. Empirical studies that compare these techniques have reported the relative effectiveness of four existing techniques on a set of subjects. These studies compare the rankings that the techniques compute for statements in the subject programs and the effectiveness of these rankings in locating the faults. However, it is unknown how these four techniques compare with Tarantula, another existing fault-localization technique, although this technique also provides a way to rank statements in terms of their suspiciousness. Thus, we performed a study to compare the Tarantula technique with the four techniques previously compared. This paper presents our study---it overviews the Tarantula technique along with the four other techniques studied, describes our experiment, and reports and discusses the results. Our studies show that, on the same set of subjects, the Tarantula technique consistently outperforms the other four techniques in terms of effectiveness in fault localization, and is comparable in efficiency to the least expensive of the other four techniques.},
author = {Jones, J.a. James a and Harrold, Mary Jean M.J.},
doi = {http://doi.acm.org/10.1145/1101908.1101949},
file = {:home/aduarte/Downloads/jones05.pdf:pdf},
isbn = {1581139934},
journal = {Automated Software Engineering},
keywords = {automated debugging,fault localization,program analysis},
pages = {282--292},
title = {{Empirical evaluation of the tarantula automatic fault-localization technique}},
url = {http://portal.acm.org/citation.cfm?id=1101949},
year = {2005}
}
@article{Mayer2008,
author = {Mayer, Wolfgang and Stumptner, Markus},
file = {:home/aduarte/Downloads/Evaluating Models for Model-Based Debugging.pdf:pdf},
isbn = {9781424421886},
pages = {128--137},
title = {{Evaluating Models for Model-Based Debugging.pdf}},
year = {2008}
}
@inproceedings{Gousios2012,
abstract = {A common requirement of many empirical software engineering studies is the acquisition and curation of data from software repositories. During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve both the commits to the projects' repositories and events generated through user actions on project resources. GHTorrent aims to create a scalable off line mirror of GitHub's event streams and persistent data, and offer it to the research community as a service. In this paper, we present the project's design and initial implementation and demonstrate how the provided datasets can be queried and processed.},
author = {Gousios, Georgios and Spinellis, D.},
booktitle = {2012 9th IEEE Working Conference on Mining Software Repositories (MSR)},
doi = {10.1109/MSR.2012.6224294},
isbn = {978-1-4673-1761-0},
issn = {2160-1852},
keywords = {Communities,Distributed databases,Electronic mail,GHTorrent,GitHub,Github data,Organizations,Peer to peer computing,Protocols,REST API,application program interfaces,collaboration platform,commits,data acquisition,data curation,dataset,events,mirroring platform,open source software,project hosting platform,project resources,public domain software,repository,software engineering,software engineering studies,software repositories,storage management,user actions},
month = {jun},
pages = {12--21},
publisher = {IEEE},
shorttitle = {Mining Software Repositories (MSR), 2012 9th IEEE },
title = {{GHTorrent: Github's data from a firehose}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6224294},
year = {2012}
}
@inproceedings{Campos2012,
address = {New York, New York, USA},
author = {Campos, Jos{\'{e}} and Riboira, Andr{\'{e}} and Perez, Alexandre and Abreu, Rui},
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering - ASE 2012},
doi = {10.1145/2351676.2351752},
isbn = {9781450312042},
keywords = {Automatic Debugging,Automatic Testing,Eclipse plug-in,GZoltar,RZoltar},
month = {sep},
pages = {378},
publisher = {ACM Press},
title = {{GZoltar: an eclipse plug-in for testing and debugging}},
url = {http://dl.acm.org/citation.cfm?id=2351676.2351752},
year = {2012}
}
@article{Mockus2000,
abstract = {Large scale software products must constantly change in order to adapt to a changing environment. Studies of historic data from legacy software systems have identified three specific causes of this change: adding new features; correcting faults; and restructuring code to accommodate future changes. Our hypothesis is that a textual description field of a change is essential to understanding why that change was performed. Also, we expect that difficulty, size, and interval would vary strongly across different types of changes. To test these hypotheses we have designed a program which automatically classifies maintenance activity based on a textual description of changes. Developer surveys showed that the automatic classification was in agreement with developer opinions. Tests of the classifier on a different product found that size and interval for different types of changes did not vary across two products. We have found strong relationships between the type and size of a change and the time required to carry it out. We also discovered a relatively large amount of perfective changes in the system we examined. From this study we have arrived at several suggestions on how to make version control data useful in diagnosing the state of a software project, without significantly increasing the overhead for the developer using the change management system},
author = {Mockus, a. and Votta, L.G.},
doi = {10.1109/ICSM.2000.883028},
file = {:home/aduarte/Downloads/ReasonforChange{\_}ICSM2000.pdf:pdf},
isbn = {0-7695-0753-0},
issn = {1063-6773},
journal = {ICSM conf.},
pages = {120--130},
title = {{Identifying reasons for software changes using historic databases}},
year = {2000}
}
@article{Dallmeier2005,
abstract = { A common method to localize defects is to compare the coverage of passing and failing program runs: A method executed only in failing runs, for instance, is likely to point to the defect. However, some failures, occur only after a specific sequence of method calls, such as multiple deallocations of the same resource. Such sequences can be collected from arbitrary Java programs at low cost; comparing object-specific sequences predicts defects better than simply comparing coverage. In a controlled experiment, our technique pinpointed the defective class in 39{\%} of all test runs.  Title：Lightweight Defect Localization for Java  题目：轻量级Java故障检测 检测故障的常见做法是比较程序成功和失败执行的覆盖：例如，一种方法是失败执行覆盖的部分可能包含着故障。然而，有些故障只有按特定序列执行方法时才回发生，例如，多次释放某些资源。这些执行序列可以低代价的从任意Java程序中收集，与比较覆盖相比，比较特定对象序列可以更好的预测故障。在一次受控实验中，我们的技术找出了39{\%}的测试中的故障。注意： 本文提出的比较特定对象序列的前提是“要预先知道这些对象的序列可以引发故障”（待验证？）},
annote = {AMPLE},
author = {Dallmeier, Valentin and Lindig, Christian and Zeller, Andreas},
doi = {10.1007/11531142{\_}23},
file = {:home/aduarte/Downloads/ecoop-05.pdf:pdf},
isbn = {978-3-540-27992-1},
issn = {03029743},
journal = {ECOOP 2005 - Object-Oriented Programming,Proceedings},
pages = {528--550},
title = {{Lightweight Defect Localization for Java }},
url = {http://dx.doi.org/10.1007/11531142{\_}23},
volume = {3586},
year = {2005}
}
@misc{Carlsson2013,
abstract = {When performing an analysis of the evolution of software quality and software metrics,there is a need to get access to as many versions of the source code as possible. There isa lack of research on ...},
author = {Carlsson, Emil},
file = {:home/aduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Carlsson - 2013 - Mining Git Repositories An introduction to repository mining.pdf:pdf},
keywords = {Computer Science,Datavetenskap (datalogi)},
language = {eng},
title = {{Mining Git Repositories : An introduction to repository mining}},
url = {http://www.diva-portal.org/smash/record.jsf?pid=diva2{\%}3A638844{\&}dswid=4999},
year = {2013}
}
@article{Abreu2007,
abstract = {Spectrum-based fault localization shortens the test- diagnose-repair cycle by reducing the debugging effort. As a light-weight automated diagnosis technique it can easily be integrated with existing testing schemes. However, as no model of the system is taken into account, its diagnostic accuracy is inherently limited. Using the Siemens Set benchmark, we investigate this diagnostic accuracy as a function of several parameters (such as quality and quantity of the program spectra collected during the execution of the system), some of which directly relate to test design. Our results indicate that the superior performance of a particular similarity coefficient, used to analyze the program spectra, is largely independent of test design. Furthermore, near- optimal diagnostic accuracy (exonerating about 80{\%} of the blocks of code on average) is already obtained for low-quality error observations and limited numbers of test cases. The influence of the number of test cases is of primary importance for continuous (embedded) processing applications, where only limited observation horizons can be maintained.},
author = {Abreu, Rui and Zoeteweij, Peter and {Van Gemund}, Arjan J C},
doi = {10.1109/TAICPART.2007.4344104},
file = {:home/aduarte/Downloads/1750.pdf:pdf},
isbn = {0769529844},
journal = {Proceedings - Testing: Academic and Industrial Conference Practice and Research Techniques, TAIC PART-Mutation 2007},
keywords = {Program spectra,Software fault diagnosis,Test data analysis},
pages = {89--98},
title = {{On the accuracy of spectrum-based fault localization}},
year = {2007}
}
@article{Chen2002,
abstract = { Traditional problem determination techniques rely on static dependency models that are difficult to generate accurately in today's large, distributed, and dynamic application environments such as e-commerce systems. We present a dynamic analysis methodology that automates problem determination in these environments by 1) coarse-grained tagging of numerous real client requests as they travel through the system and 2) using data mining techniques to correlate the believed failures and successes of these requests to determine which components are most likely to be at fault. To validate our methodology, we have implemented Pinpoint, a framework for root cause analysis on the J2EE platform that requires no knowledge of the application components. Pinpoint consists of three parts: a communications layer that traces client requests, a failure detector that uses traffic-sniffing and middleware instrumentation, and a data analysis engine. We evaluate Pinpoint by injecting faults into various application components and show that Pinpoint identifies the faulty components with high accuracy and produces few false-positives.},
author = {Chen, Mike Y. and Kiciman, Emre and Fratkin, Eugene and Fox, Armando and Brewer, Eric},
doi = {10.1109/DSN.2002.1029005},
file = {:home/aduarte/Downloads/ipds2002{\_}pinpoint.pdf:pdf},
isbn = {0769515975},
journal = {Proceedings of the 2002 International Conference on Dependable Systems and Networks},
keywords = {Data clustering,Data mining algorithms,Problem determination,Problem diagnosis,Root cause analysis},
pages = {595--604},
title = {{Pinpoint: Problem determination in large, dynamic internet services}},
year = {2002}
}
@article{Kim2007,
abstract = {We analyze the version history of 7 software systems to predict the most fault prone entities and files. The basic assumption is that faults do not occur in isolation, but rather in bursts of several related faults. Therefore, we cache locations that are likely to have faults: starting from the location of a known (fixed) fault, we cache the location itself, any locations changed together with the fault, recently added locations, and recently changed locations. By consulting the cache at the moment a fault is fixed, a developer can detect likely fault-prone locations. This is useful for prioritizing verification and validation resources on the most fault prone files or entities. In our evaluation of seven open source projects with more than 200,000 revisions, the cache selects 10{\%} of the source code files; these files account for 73{\%}-95{\%} of faults - a significant advance beyond the state of the art.},
annote = {BugCache + FixCache},
author = {Kim, Sunghun and Zimmermann, Thomas and Whitehead, E. James and Zeller, Andreas},
doi = {10.1109/ICSE.2007.66},
file = {:home/aduarte/Dropbox/Disserta{\c{c}}{\~{a}}o/Research/Predicting Faults from Cached History.pdf:pdf},
isbn = {0769528287},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
pages = {489--498},
title = {{Predicting faults from cached history}},
year = {2007}
}
@article{Weiser1981,
author = {Weiser, Mark},
isbn = {0-89791-146-6},
keywords = {Data flow analysis,Debugging,Human factors,Program maintenance,Program metrics,Software tools},
month = {mar},
pages = {439--449},
publisher = {IEEE Press},
title = {{Program slicing}},
url = {http://dl.acm.org/citation.cfm?id=800078.802557},
year = {1981}
}
@article{Weiser1982,
abstract = {Computer programmers break apart large programs into smaller coherent pieces. Each of these pieces: functions, subroutines, modules, or asbtract datatypes, is usually a contiguous piece of programmers text. The experiment reported here shows that programmers also routinely break programs into one kind of coherent piece which is not contigious. When debugging unfamiliar programs programmers use program pieces called slices which are sets of statements related by their flow of data. The statements in a slice are not necessarily textually contiguous, but may be scattered through a program.},
author = {Weiser, Mark},
doi = {10.1145/358557.358577},
file = {:home/aduarte/Downloads/Weiser82SlicingClassic.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {7},
pages = {446--452},
title = {{Programmers use slices when debugging}},
volume = {25},
year = {1982}
}
@article{Abreu2011,
abstract = {(Semi-)automated diagnosis of software faults can drastically increase debugging efficiency, improving reliability and time-to-market. Current automatic diagnosis techniques are predominantly of a statistical nature and, despite typical defect densities, do not explicitly consider multiple faults, as also demonstrated by the popularity of the single-fault benchmark set of programs. We present a reasoning approach, called Zoltar-M(ultiple fault), that yields multiple-fault diagnoses, ranked in order of their probability. Although application of Zoltar-M to programs with many faults requires heuristics (trading-off completeness) to reduce the inherent computational complexity, theory as well as experiments on synthetic program models and multiple-fault program versions available from the software infrastructure repository (SIR) show that for multiple-fault programs this approach can outperform statistical techniques, notably spectrum-based fault localization (SFL). As a side-effect of this research, we present a new SFL variant, called Zoltar-S(ingle fault), that is optimal for single-fault programs, outperforming all other variants known to date. © 2010 Elsevier Inc. All rights reserved.},
annote = {Zoltar-M},
author = {Abreu, Rui and Zoeteweij, Peter and {Van Gemund}, Arjan J C},
doi = {10.1016/j.jss.2010.11.915},
file = {:home/aduarte/Dropbox/Disserta{\c{c}}{\~{a}}o/Research/Abreu, R. - Simultaneous Debugging of Software Faults.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Program spectra,Software fault diagnosis,Statistical and reasoning approaches},
number = {4},
pages = {573--586},
title = {{Simultaneous debugging of software faults}},
url = {http://dx.doi.org/10.1016/j.jss.2010.11.915},
volume = {84},
year = {2011}
}
@article{Abreu2009,
abstract = {Fault diagnosis approaches can generally be categorized into spectrum-based fault localization (SFL, correlating failures with abstractions of program traces), and model-based diagnosis (MBD, logic reasoning over a behavioral model). Although MBD approaches are inherently more accurate than SFL, their high computational complexity prohibits application to large programs. We present a framework to combine the best of both worlds, coined BARINEL. The program is modeled using abstractions of program traces (as in SFL) while Bayesian reasoning is used to deduce multiple-fault candidates and their probabilities (as in MBD). A particular feature of BARINEL is the usage of a probabilistic component model that accounts for the fact that faulty components may fail intermittently. Experimental results on both synthetic and real software programs show that BARINEL typically outperforms current SFL approaches at a cost complexity that is only marginally higher. In the context of single faults this superiority is established by formal proof.},
annote = {Barinel!!},
author = {Abreu, Rui and Zoeteweij, P and Gemund, a J C Van},
doi = {10.1109/ASE.2009.25},
file = {:home/aduarte/Dropbox/Disserta{\c{c}}{\~{a}}o/Research/Abreu, R. - Spectrum-based multiple fault localization.pdf:pdf},
isbn = {978-1-4244-5259-0},
issn = {1938-4300},
journal = {Automated Software Engineering 2009 ASE 09 24th IEEEACM International Conference on},
keywords = {program spectra,software fault diagnosis,statistical reasoning approaches},
pages = {88--99},
title = {{Spectrum-Based Multiple Fault Localization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5431781},
year = {2009}
}
@article{Reps1997,
abstract = {This paper describes new techniques to help with testing and debugging, using information obtained from path profiling. Apath profiler instruments a program so that the number of times each different loopfree path executes is accumulated during an execution run. With such an instrumented program, each run of the program generates a path spectrum for the execution—a distribution of the paths that were executed during that run. Apath spectrum is a finite, easily obtainable characterization of a program’s execution on adataset, and provides a behavior signature for a run of the program. Our techniques are based on the idea of comparing path spectra from different runs of the program. When different runs produce different spectra, the spectral differences can be used to identify paths in the program along which control diverges in the two runs. By choosing input datasets to hold all factors constant except one, the divergence can be attributed to this factor. The point of divergence itself may not be the cause of the underlying problem, but provides a starting place for a programmer to begin his exploration. One application of this technique is in the “Year 2000 Problem ” (i.e., the problem of fixing computer systems that use only 2-digit year fields in date-valued data). In this context, path-spectrum comparison provides a heuristic for identifying paths in a program that are good candidates for being date-dependent computations. The application of path-spectrum comparison to a number of other software-maintenance issues is also discussed.},
author = {Reps, Thomas and Ball, Thomas and Das, Manuvir and Larus, James},
doi = {10.1145/267896.267925},
file = {:home/aduarte/Downloads/y2k.pdf:pdf},
isbn = {3-540-63531-9},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
number = {6},
pages = {432--449},
title = {{The use of program profiling for software maintenance with applications to the year 2000 problem}},
volume = {22},
year = {1997}
}
@article{Sliwerski2005,
author = {{\'{S}}liwerski, Jacek and Zimmermann, Thomas and Zeller, Andreas},
doi = {10.1145/1082983.1083147},
isbn = {1-59593-123-6},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
month = {jul},
number = {4},
pages = {1},
publisher = {ACM},
title = {{When do changes induce fixes?}},
url = {http://dl.acm.org/citation.cfm?id=1082983.1083147},
volume = {30},
year = {2005}
}
@inproceedings{Janssen2009,
abstract = {Locating software components which are responsible for ob- served failures is the most expensive, error-prone phase in the software development life cycle. Automated diagnosis of software faults can improve the efficiency of the debugging process, and is therefore an important process for the de- velopment of dependable software. In this paper we present a toolset for automatic fault localization, dubbed Zoltar, which adopts a spectrum-based fault localization technique. The toolset provides the infrastructure to automatically in- strument the source code of software programs to produce runtime data, which is subsequently analyzed to return a ranked list of likely faulty locations. Aimed at total au- tomation (e.g., for runtime fault diagnosis), Zoltar has the capability of instrumenting the program under analysis with fault screeners, for automatic error detection. Using a small thread-based example program as well as a large realistic program, we show the applicability of the proposed toolset.},
annote = {Zoltar},
author = {Janssen, T and Gemund, A and Abreu, Rui},
booktitle = {Instrumentation},
doi = {10.1145/1596495.1596502},
file = {:home/aduarte/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Janssen, Gemund, Abreu - 2009 - Zoltar A Spectrum-based Fault Localization Tool.pdf:pdf},
isbn = {9781605586816},
pages = {23--29},
title = {{Zoltar: A Spectrum-based Fault Localization Tool}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-70450260945{\&}partnerID=40{\&}md5=7f1ad847cdab93c6b6c96d1e60e1ae78},
year = {2009}
}
@article{Zhivich2009,
abstract = {Software is no longer creeping into every aspect of our lives - it's already there. In fact, failing to recognize just how much everything we do depends on software functioning correctly makes modern society vulnerable to software errors.},
author = {Zhivich, Michael and Cunningham, Robert K.},
doi = {10.1109/MSP.2009.56},
file = {:home/aduarte/Downloads/74607 (3).pdf:pdf},
isbn = {1521-9615},
issn = {15407993},
journal = {IEEE Security and Privacy},
keywords = {Cost,Risks,Secure systems,Software errors,Software failures},
number = {2},
pages = {87--90},
title = {{The real cost of software errors}},
volume = {7},
year = {2009}
}
